// Please do not change the order of the numbered sections!
// The expected order is: 1. Parameters - 2. Profiles - 3. Process -  4. Executor
// There is a high chance it would break the configuration of 'profiles'

manifest {
    name            = 'cynapse-ccri/cgpwgs-nf'
    homePage        = 'https://github.com/cynapse-ccri/cgpwgs-nf'
    description     = 'Nextflow implementation of cancerit/dockstore-cgpwgs (without bash script)'
    mainScript      = 'main.nf'
    version         = '0.0.0'
}

// 1. Parameters

// NOTE:
// Initialise the values of the params to the preferred default value or to false
params {
    // output folder options
    outdir          = 'results'
    tracedir        = "${params.outdir}/pipeline_info"

    // client-specific variables to be used in pipeline introspection reporting
    raci_owner      = "NA"
    domain_keywords = "NA"

    // inputs, workflow specific
    pairs        = null
    // reference sets
    core_ref     = null // core_ref_*.tar.gz
    snv_indel    = null // SNV_INDEL_ref_*-fragment.tar.gz
    cvn_sv       = null // CNV_SV_ref_*_brass6+.tar.gz
    annot        = null // VAGrENT_ref_*_ensembl_*.tar.gz
    qc_genotype  = null // qcGenotype_*.tar.gz
    // options
    exclude      = 'MT,NC_007605,hs37d5,GL%'  // csv string of contigs (pindel/caveman)
    exfile       = 'NO_FILE'  // cannot use null on optional file
    skipgerm     = false  // true/false option (pindel)
    cavereads    = 700000 // refinment may be necessary for GRCh37 vs GRCh38
    cavevcfsplit = 25000  // number of records to split caveman VCF file into for flagging

    // default cpus
    cpus_caveman = 30
    cpus_pindel = 6
    cpus_brass = 6
    cpus_counts = 3

    // report_dir is:
    // - the folder from the container that includes the scripts for NF <= v20.01 (bin)
    // - the ${projectDir}/bin folder of the root of the repo with the scripts for NF >= v20.10
    report_dir = '/opt/bin/'

    // when set to true, prints help and exits
    help = false

    // default container for all processes, excluding those defined differently via the usage with 'withName'
    container = 'quay.io/lifebitai/ubuntu:18.10'

    // process resources defaults
    cpus   = 1
    memory = 1.GB
    disk   = '30.GB'

    // max resources limits defaults
    max_cpus   = 32
    max_memory = 192.GB
    max_time   = 8.h

    // execution related defaults
    config        = 'conf/standard.config'
    echo          = false
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'terminate' }
    maxRetries    = 9
    maxForks      = 200
    queueSize     = 200
    executor      = false

    // google-lifesciences
    gls_bootDiskSize = '50.GB'
    gls_preemptible  = true
    zone             = 'us-east1-b'
    network          = 'default'
    subnetwork       = 'default'
}

// 2. Profiles


// Do not update the order because the values set in params scope will not be overwritten
// Do not attempt to simplify to
// includeConfig params.config
// outside of profiles scope, it will fail to update the values of the params
profiles {
    standard {includeConfig params.config}
    docker { docker.enabled = true }
    base {includeConfig 'conf/base.config'}
    google {includeConfig 'conf/google.config'}
    test {includeConfig 'conf/test.config'} // must be before singularity
    local {includeConfig 'conf/test.config'}
    singularity {includeConfig 'conf/singularity.config'}
    slurm {includeConfig 'conf/slurm.config'}
}

// 3. Process

// Do not change order of block, must follow after profiles scope (last section that updates params)
process {
    echo          = params.echo
    cpus          = params.cpus
    memory        = params.memory
    maxRetries    = params.maxRetries
    maxForks      = params.maxForks
    container     = params.container
    errorStrategy = params.errorStrategy

    withName: obtain_pipeline_metadata {
        executor = 'local'
        container = null
    }

    withName: prep_ref {
      container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: ascat_counts {
        time      = '2h'
        cpus      = { workflow.stubRun ? 1  : params.cpus_counts }
        memory    = { workflow.stubRun ? 1.GB  : 1.GB * task.cpus }
        //cpus      = {check_stub( params.cpus_counts, 'cpus')}
        //memory    = {check_stub( { 1.GB * task.cpus }, 'memory')}
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: ascat {
        time      = '2h'
        cpus      = { workflow.stubRun ? 1 : params.cpus_counts }
        memory    = { workflow.stubRun ? 1.GB : 5.GB }
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: pindel {
        time      = '12h'
        cpus      = { workflow.stubRun ? 1 : params.cpus_counts }
        memory    =  {workflow.stubRun ? 1.GB :  6.GB * task.cpus }
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: pindel_flag {
        time      = '4h'
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: vagrent {
        time      = '1h'
        container = "quay.io/wtsicgp/vagrent:v3.7.0"
    }

    withName: caveman {
        // want something like 32/244 or 64/488 (cpu/GB) - r4.17xlarge (6-10 GB/core)
        // must be r/m flavour type of hosts, need to clarify how this will work
        time      = '12h'
        cpus      = { workflow.stubRun ? 1 : params.cpus_counts }
        memory    = { workflow.stubRun ? 1.GB : 5.GB * task.cpus }
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_vcf_split {
        time      = '10m'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_flag {
        time      = '2h'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_flag_merge {
        time      = '10m'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: brass {
        time      = '12h'
        cpus      = { workflow.stubRun ? 1 : params.cpus_counts }
        memory    = { workflow.stubRun ? 1.GB : 6.GB * task.cpus }
        container = "quay.io/wtsicgp/brass:v6.3.4"
    }

    withName: genotypes {
        time      = '30m'
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: verifybamid {
        time      = '12h'
        cpus      = { workflow.stubRun ? 1 : 2 }
        memory    = { workflow.stubRun ? 1.GB : 6.GB }
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    // // Allocating CPUs and memory in a more flexible manner to above
    // // Attempt with 8 CPUs and 12GB of memory first
    // // If your pipeline doesn't include a process named 'step_2', update the name to match your process of choice or delete this block
    // withName: step_2 {
    //     cpus   = {check_max(8 * task.attempt, 'cpus')}
    //     memory = {check_max(12.GB * task.attempt, 'memory')}
    // }

    // // If your pipeline doesn't include a process named 'report', update the name to match your process of choice or delete this block
    // withName: report {
    //     cpus      = params.cpus
    //     memory    = params.memory
    //     container = 'quay.io/lifebitaiorg/report:poppler'
    // }
}

// 4. Executor - Do not remove this section! Required for running with different executors using --executor parameter

executor {
    name      = params.executor
    queueSize = params.queueSize
}

// 5. Nextflow timeline, report and trace reports

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    fields  = 'task_id,native_id,process,name,tag,status,exit,cpus,%cpu,memory,%mem,rss,vmem,peak_rss,peak_rss,container'
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
