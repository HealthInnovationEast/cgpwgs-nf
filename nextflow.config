// Please do not change the order of the numbered sections!
// The expected order is: 1. Parameters - 2. Profiles - 3. Process -  4. Executor
// There is a high chance it would break the configuration of 'profiles'

manifest {
    name            = 'HealthInnovationEast/cgpwgs-nf'
    homePage        = 'https://github.com/HealthInnovationEast/cgpwgs-nf'
    description     = 'Nextflow implementation of cancerit/dockstore-cgpwgs (without bash script)'
    mainScript      = 'main.nf'
    version         = '0.0.0'
}

// 1. Parameters

// NOTE:
// Initialise the values of the params to the preferred default value or to false
params {
    // output folder options
    outdir          = 'results'
    tracedir        = "${params.outdir}/pipeline_info"

    // client-specific variables to be used in pipeline introspection reporting
    raci_owner      = "NA"
    domain_keywords = "NA"

    // inputs, workflow specific
    pairs        = null
    // reference sets
    core_ref     = null // core_ref_*.tar.gz
    snv_indel    = null // SNV_INDEL_ref_*-fragment.tar.gz
    cnv_sv       = null // CNV_SV_ref_*_brass6+.tar.gz
    annot        = null // VAGrENT_ref_*_ensembl_*.tar.gz
    qc_genotype  = null // qcGenotype_*.tar.gz
    // options
    exclude      = 'MT,NC_007605,hs37d5,GL%'  // csv string of contigs (pindel/caveman)
    exfile       = "${baseDir}/assets/null/NO_FILE"  // cannot use null on optional file
    skipgerm     = false  // true/false option (pindel)
    cavereads    = 700000 // refinment may be necessary for GRCh37 vs GRCh38
    cavevcfsplit = 15000  // number of records to split caveman VCF file into for flagging

    // default cpus
    cpus_caveman = 64 // needs to roughtly align with cost-saving/spot hosts
    cpus_pindel = 8
    cpus_brass = 4
    cpus_counts = 2

    // report_dir is:
    // - the folder from the container that includes the scripts for NF <= v20.01 (bin)
    // - the ${projectDir}/bin folder of the root of the repo with the scripts for NF >= v20.10
    report_dir = '/opt/bin/'

    // when set to true, prints help and exits
    help = false

    // default container for all processes, excluding those defined differently via the usage with 'withName'
    container = 'quay.io/lifebitai/ubuntu:18.10'

    // process resources defaults
    cpus   = 1
    memory = 4.GB

    // max resources limits defaults
    max_cpus   = 192
    max_memory = 768.GB
    max_time   = 7.d

    // execution related defaults
    config        = 'conf/standard.config'
    echo          = false
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'terminate' }
    maxRetries    = 3
    maxForks      = 200
    queueSize     = 200
    executor      = 'awsbatch'
}

// 2. Profiles


// Do not update the order because the values set in params scope will not be overwritten
// Do not attempt to simplify to
// includeConfig params.config
// outside of profiles scope, it will fail to update the values of the params
profiles {
    standard {includeConfig params.config}
    docker { docker.enabled = true }
    base {includeConfig 'conf/base.config'}
    awsbatch {includeConfig 'conf/awsbatch.config'}
    test {includeConfig 'conf/test.config'} // must be before singularity
    local {includeConfig 'conf/local.config'}
    singularity {includeConfig 'conf/singularity.config'}
    slurm {includeConfig 'conf/slurm.config'}
}

// 3. Process

// Do not change order of block, must follow after profiles scope (last section that updates params)
process {
    echo          = params.echo
    cpus          = params.cpus
    memory        = params.memory
    maxRetries    = params.maxRetries
    maxForks      = params.maxForks
    container     = params.container
    errorStrategy = params.errorStrategy

    // withName: obtain_pipeline_metadata {
    //     executor = 'local'
    //     container = null
    // }

    withName: prep_ref {
      time      = '2h'
      container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: genotypes {
        time      = '2h'
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
        memory    = 6.GB
    }

    withName: ascat_counts {
        time      = '6h'
        cpus = { multiply_check_max(params.cpus_counts, 1, 'cpus') }
        memory = { multiply_check_max(2.GB, 0.8 + (task.attempt * 0.2), 'memory') }
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: ascat {
        time      = '12h'
        memory    = { multiply_check_max(8.GB, 0.8 + (task.attempt * 0.2), 'memory') }
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: pindel {
        time      = '36h'
        cpus      = { multiply_check_max(params.cpus_pindel, 1, 'cpus') }
        memory    =  { multiply_check_max(6.GB, task.cpus * (0.8 + (task.attempt * 0.2)), 'memory') }
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: pindel_flag {
        time      = '6h'
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: caveman {
        // want something like 32/244 or 64/488 (cpu/GB) - r4.17xlarge (6-10 GB/core)
        // must be r/m flavour type of hosts, need to clarify how this will work
        time      = '48h'
        cpus      = { multiply_check_max(params.cpus_caveman, 1, 'cpus') }
        memory    = { multiply_check_max(5.5.GB, task.cpus * (0.8 + (task.attempt * 0.2)), 'memory') }
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.3"
    }

    withName: caveman_flag {
        time      = '12h'
        memory    = { multiply_check_max(6.GB, 0.8 + (task.attempt * 0.2), 'memory') }
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.3"
    }

    withName: caveman_flag_merge {
        time      = '1h'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.3"
    }

    withName: vagrent {
        time      = '2h'
        container = "quay.io/wtsicgp/vagrent:v3.7.0"
    }

    withName: verifybamid {
        time      = '3h'
        cpus      = 2
        memory    = 6.GB
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: brass {
        time      = '48h'
        cpus      = { multiply_check_max(params.cpus_brass, 1, 'cpus') }
        memory    = { multiply_check_max(6.GB, 0.8 + (task.attempt * 0.2), 'memory') }
        container = "quay.io/wtsicgp/brass:v6.3.4"
    }
}

// 4. Executor - Do not remove this section! Required for running with different executors using --executor parameter

executor {
    name      = params.executor
    queueSize = params.queueSize
}

// 5. Nextflow timeline, report and trace reports

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    fields  = 'task_id,native_id,process,name,tag,status,exit,cpus,%cpu,memory,%mem,rss,vmem,peak_rss,peak_rss,container'
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}

def multiply_check_max(obj, multi, type) {
    /**
     * See multiply_check_max(obj, multi, type, alt_max)
     */
    return multiply_check_max(obj, multi, type, null)
}

def multiply_check_max(obj, multi, type, alt_max) {
    if (type == 'memory') {
        if (obj instanceof nextflow.util.MemoryUnit) {
            obj = obj * multi
        }
        else {
            obj = new nextflow.util.MemoryUnit(obj) * multi
        }
        try {
            max_memory = alt_max == null ? params.max_memory : alt_max
            if (obj.compareTo(max_memory as nextflow.util.MemoryUnit) == 1)
                return max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        max_time = alt_max == null ? params.max_time : alt_max
        if (obj instanceof nextflow.util.Duration) {
            obj = obj * multi
        }
        else {
            obj = new nextflow.util.Duration(obj) * multi
        }
        try {
            if (obj.compareTo(max_time as nextflow.util.Duration) == 1)
                return max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        max_cpus = alt_max == null ? params.max_cpus : alt_max
        obj = obj * multi
        try {
            return Math.min(obj, max_cpus as int)
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
