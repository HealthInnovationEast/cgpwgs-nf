// Please do not change the order of the numbered sections!
// The expected order is: 1. Parameters - 2. Profiles - 3. Process -  4. Executor
// There is a high chance it would break the configuration of 'profiles'

manifest {
    name            = 'cynapse-ccri/cgpwgs-nf'
    homePage        = 'https://github.com/cynapse-ccri/cgpwgs-nf'
    description     = 'Nextflow implementation of cancerit/dockstore-cgpwgs (without bash script)'
    mainScript      = 'main.nf'
    version         = '0.0.0'
}

// 1. Parameters

// NOTE:
// Initialise the values of the params to the preferred default value or to false
params {
    // output folder options
    outdir          = 'results'
    tracedir        = "${params.outdir}/pipeline_info"

    // client-specific variables to be used in pipeline introspection reporting
    raci_owner      = "NA"
    domain_keywords = "NA"

    // inputs, workflow specific
    pairs        = null
    // reference sets
    core_ref     = null // core_ref_*.tar.gz
    snv_indel    = null // SNV_INDEL_ref_*-fragment.tar.gz
    cvn_sv       = null // CNV_SV_ref_*_brass6+.tar.gz
    annot        = null // VAGrENT_ref_*_ensembl_*.tar.gz
    qc_genotype  = null // qcGenotype_*.tar.gz
    // options
    exclude      = 'MT,NC_007605,hs37d5,GL%'  // csv string of contigs (pindel/caveman)
    exfile       = 'NO_FILE'  // cannot use null on optional file
    skipgerm     = false  // true/false option (pindel)
    cavereads    = 700000 // refinment may be necessary for GRCh37 vs GRCh38
    cavevcfsplit = 25000  // number of records to split caveman VCF file into for flagging

    // default cpus
    cpus_caveman = 30
    cpus_pindel = 6
    cpus_brass = 6
    cpus_counts = 3

    // report_dir is:
    // - the folder from the container that includes the scripts for NF <= v20.01 (bin)
    // - the ${projectDir}/bin folder of the root of the repo with the scripts for NF >= v20.10
    report_dir = '/opt/bin/'

    // when set to true, prints help and exits
    help = false

    // default container for all processes, excluding those defined differently via the usage with 'withName'
    container = 'quay.io/lifebitai/ubuntu:18.10'

    // process resources defaults
    cpus   = 1
    memory = 0.5.GB
    disk   = '30.GB'

    // max resources limits defaults
    max_cpus   = 192
    max_memory = 768.GB
    max_time   = 7.d

    // execution related defaults
    config        = 'conf/standard.config'
    echo          = false
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'terminate' }
    maxRetries    = 3
    maxForks      = 200
    queueSize     = 200
    executor      = false

    // google-lifesciences
    gls_bootDiskSize = '50.GB'
    gls_preemptible  = true
    zone             = 'us-east1-b'
    network          = 'default'
    subnetwork       = 'default'
}

// 2. Profiles


// Do not update the order because the values set in params scope will not be overwritten
// Do not attempt to simplify to
// includeConfig params.config
// outside of profiles scope, it will fail to update the values of the params
profiles {
    standard {includeConfig params.config}
    docker { docker.enabled = true }
    base {includeConfig 'conf/base.config'}
    awsbatch {includeConfig 'conf/awsbatch.config'}
    google {includeConfig 'conf/google.config'}
    test {includeConfig 'conf/test.config'} // must be before singularity
    local {includeConfig 'conf/test.config'}
    singularity {includeConfig 'conf/singularity.config'}
    slurm {includeConfig 'conf/slurm.config'}
}

// 3. Process

// Do not change order of block, must follow after profiles scope (last section that updates params)
process {
    echo          = params.echo
    cpus          = params.cpus
    memory        = params.memory
    maxRetries    = params.maxRetries
    maxForks      = params.maxForks
    container     = params.container
    errorStrategy = params.errorStrategy

    withName: obtain_pipeline_metadata {
        executor = 'local'
        container = null
    }

    withName: prep_ref {
      time      = '1h'
      container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: ascat_counts {
        time      = '2h'
        cpus      = { workflow.stubRun ? 1  : resource_max(params.cpus_counts) }
        memory    = { workflow.stubRun ? 1.GB  : resource_max(0.5.GB * task.cpus) }
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: ascat {
        time      = '2h'
        memory    = { workflow.stubRun ? 1.GB : resource_max(8.GB * task.attempt) } // 8GB is max available for 1 CPU host in AWS
        container = "quay.io/wtsicgp/ascatngs:4.5.0"
    }

    withName: pindel {
        time      = '12h'
        cpus      = { workflow.stubRun ? 1 : resource_max(params.cpus_pindel) }
        memory    =  {workflow.stubRun ? 1.GB :  resource_max(6.GB * task.cpus) }
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: pindel_flag {
        time      = '4h'
        container = "quay.io/wtsicgp/cgppindel:3.10.0"
    }

    withName: vagrent {
        time      = '1h'
        container = "quay.io/wtsicgp/vagrent:v3.7.0"
    }

    withName: caveman {
        // want something like 32/244 or 64/488 (cpu/GB) - r4.17xlarge (6-10 GB/core)
        // must be r/m flavour type of hosts, need to clarify how this will work
        time      = '36h'
        cpus      = { workflow.stubRun ? 1 : resource_max(params.cpus_caveman) }
        memory    = { workflow.stubRun ? 1.GB : resource_max(6.GB * task.cpus * task.attempt) }
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_vcf_split {
        time      = '10m'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_flag {
        time      = '2h'
        memory    = { workflow.stubRun ? 1.GB : resource_max(2.GB * task.attempt) }
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: caveman_flag_merge {
        time      = '10m'
        container = "quay.io/wtsicgp/cgpcavemanwrapper:1.18.2"
    }

    withName: brass {
        time      = '36h'
        cpus      = { workflow.stubRun ? 1 : resource_max(params.cpus_brass) }
        memory    = { workflow.stubRun ? 1.GB : resource_max(6.GB * task.cpus) }
        container = "quay.io/wtsicgp/brass:v6.3.4"
    }

    withName: genotypes {
        time      = '30m'
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }

    withName: verifybamid {
        time      = '2h'
        cpus      = { workflow.stubRun ? 1 : 2 }
        memory    = { workflow.stubRun ? 1.GB : 6.GB }
        container = "quay.io/wtsicgp/dockstore-cgpwgs:2.1.1"
    }
}

// 4. Executor - Do not remove this section! Required for running with different executors using --executor parameter

executor {
    name      = params.executor
    queueSize = params.queueSize
}

// 5. Nextflow timeline, report and trace reports

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    fields  = 'task_id,native_id,process,name,tag,status,exit,cpus,%cpu,memory,%mem,rss,vmem,peak_rss,peak_rss,container'
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}

def resource_max(obj_in) {
    /**
     * See resource_max(obj_in, alt_max)
     */
    return resource_max(obj_in, null)
}

def resource_max(obj_in, alt_max) {
    /**
     * Check resource isn't over max, optionally override the global max
     *
     * @param obj nextflow.util.MemoryUnit, nextflow.util.Duration or int
     * @param obj null or value lower than global max for matching data type as first element
     * @return obj Lower value of input and appropriate params.max_* (or alt_max)
     *
     * Only use for CPU, Memory or Duration (time)
     * Expects params for the following to exist:
     * - params.max_memory
     * - params.max_time
     * - params.max_cpus
     */
    if (obj_in instanceof nextflow.util.MemoryUnit) {
        max = alt_max == null ? params.max_memory : alt_max
        if (nextflow.util.MemoryUnit.compareTo(max, obj_in) < 0) {
            return max as nextflow.util.MemoryUnit
        }
        return obj_in
    }
    if (obj_in instanceof nextflow.util.Duration) {
        max = alt_max == null ? params.max_time : alt_max
        if (nextflow.util.Duration.compareTo(max, obj_in) < 0) {
            return max as nextflow.util.Duration
        }
        return obj_in
    }
    max = alt_max == null ? params.max_cpus : alt_max
    if (max < obj_in) {
        return max as int
    }
    return obj_in
}
